{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNw47ejmb8uUZp0xaSztqF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HueyVault/study_NLPs/blob/main/11_04_generativeLLM_HF_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "snWNvn8CTDP3"
      },
      "outputs": [],
      "source": [
        "# 데이터 및 fine tunning 된 모델 셋팅\n",
        "# !ls -al\n",
        "# !unzip ./chapter_6_withvLLMme_finetuned_model.zip\n",
        "# !unzip ./chapter_6_withvLLMme_preprocess.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# 환경 변수에 따라 파일 경로 설정\n",
        "if os.getenv('KAGGLE_KERNEL_RUN_TYPE') is not None:\n",
        "    # Kaggle 환경\n",
        "    print(\"kaggle\")\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    os.environ['HF_TOKEN']= user_secrets.get_secret(\"HF_TOKEN\")\n",
        "    os.environ['OPENAI_API_KEY'] = user_secrets.get_secret(\"OPENAI_API_KEY\")\n",
        "\n",
        "    # file path\n",
        "    train_file_path =  f'/kaggle/input/house-prices-advanced-regression-techniques/train.csv'\n",
        "    test_file_path = f'/kaggle/input/house-prices-advanced-regression-techniques/test.csv'\n",
        "\n",
        "elif 'google.colab' in str(get_ipython()):\n",
        "    print(\"colab\")\n",
        "    from google.colab import userdata\n",
        "    os.environ['HF_TOKEN'] = userdata.get(\"HF_TOKEN\")\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "else:\n",
        "    # Docker 환경\n",
        "    print(\"local\")\n",
        "    train_file_path = \"../../datasets/train.csv\"\n",
        "    test_file_path = \"../../datasets/test.csv\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylOeTQXaYWtm",
        "outputId": "bc3b49ea-49b4-45da-ca52-35a8c9477071"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자연어 평가\n",
        "- EM (Extract Match) : 텍스트 매칭(정규식, 의미, 통계 매칭)\n",
        "- EX (Execution Accuracy) : 개발 환경에서 실행\n",
        "- GPT-4 활용\n"
      ],
      "metadata": {
        "id": "mvQkuTBAVHMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.40.1 bitsandbytes==0.43.1 accelerate==0.29.3 datasets==2.19.0 tiktoken==0.6.0 -qqq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyJbFbYfW4XW",
        "outputId": "fcd2c7b7-3f5b-49b3-8b21-2f0392efec3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_file_path='data/train.csv'\n",
        "df_text2sql = pd.read_csv(train_file_path)\n",
        "df_text2sql.head()"
      ],
      "metadata": {
        "id": "fxnjuVcOZd8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text2sql.loc[1,'text']"
      ],
      "metadata": {
        "id": "k8PBPzclaOtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine tuning 된 모델 이용한 응답 만들기"
      ],
      "metadata": {
        "id": "vIt_TdtGhcik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig, pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "def make_inference_pipeline(model_id):\n",
        "    # tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    # 버전 호환 문제로 삭제\n",
        "    # # 양자화(모델 사이즈 축소)\n",
        "    # quantization_config = BitsAndBytesConfig(\n",
        "    #     load_in_4bit=True,\n",
        "    #     bnb_4bit_compute_dtype=torch.float16\n",
        "    # )\n",
        "    # model\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                                 # quantization_config=quantization_config,\n",
        "                                                #  load_in_4bit=True,\n",
        "                                                #  bnb_4bit_compute_dtype=torch.float16, # 양자화 정의\n",
        "                                                 device_map='auto'\n",
        "                                                )\n",
        "\n",
        "    # pipeline : 예측 초기화 설정\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    return pipe"
      ],
      "metadata": {
        "id": "YxFMkwj4W_3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_model_id = 'otter35/yi-ko-6b-text2sql'\n",
        "hf_pipe = make_inference_pipeline(finetuning_model_id)"
      ],
      "metadata": {
        "id": "W4PZGG3SXu4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(ddl, question, query=''):\n",
        "    prompt = f\"\"\"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
        "### DDL:\n",
        "{ddl}\n",
        "### Question:\n",
        "{question}\n",
        "### SQL:\n",
        "{query}\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "CCCM9A9Hjl0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 컬럼 추출해 프롬프트 명령어 만들기\n",
        "from tqdm import tqdm\n",
        "\n",
        "for idx, row in tqdm(df_text2sql.iterrows(),\n",
        "                     total = len(df_text2sql),\n",
        "                     desc='Generating prompt'): # DataFrame row return\n",
        "    prompt_command = make_prompt(row['context'],\n",
        "                         row['question']\n",
        "                        #  row['answer']\n",
        "                        )\n",
        "    df_text2sql.loc[idx, 'prompt'] = prompt_command\n",
        "    pass\n",
        "df_text2sql.head()"
      ],
      "metadata": {
        "id": "kesEJzJkiY3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파인튜닝된 모델에 prompt로 질문과 답 얻기"
      ],
      "metadata": {
        "id": "gQ3jZn9HlhMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = df_text2sql.loc[1,'prompt']"
      ],
      "metadata": {
        "id": "dsMtakfOlpzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = hf_pipe(example, do_sample=False,\n",
        "       return_full_text=False, max_length=512, truncation=True)\n",
        "results"
      ],
      "metadata": {
        "id": "OHOZbxSxk5fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sYNhYn1flPfI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}